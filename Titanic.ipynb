{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-12T22:17:08.875083Z","iopub.execute_input":"2021-06-12T22:17:08.875435Z","iopub.status.idle":"2021-06-12T22:17:08.884717Z","shell.execute_reply.started":"2021-06-12T22:17:08.875405Z","shell.execute_reply":"2021-06-12T22:17:08.883811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This notebook contains a solution to the Kaggle competition: Titanic.","metadata":{}},{"cell_type":"markdown","source":"I will start by reading the train data and the test data. I would typically import the pandas module in order to be able to do so. But it is already imported in the provided code in the cell above. I ran the cell above and used the output to know the path from which I should read the train and test data.","metadata":{}},{"cell_type":"code","source":"X = pd.read_csv('/kaggle/input/titanic/train.csv', index_col='PassengerId')\nX_test = pd.read_csv('/kaggle/input/titanic/test.csv', index_col='PassengerId')","metadata":{"execution":{"iopub.status.busy":"2021-06-12T22:17:08.8994Z","iopub.execute_input":"2021-06-12T22:17:08.899717Z","iopub.status.idle":"2021-06-12T22:17:08.916286Z","shell.execute_reply.started":"2021-06-12T22:17:08.899688Z","shell.execute_reply":"2021-06-12T22:17:08.915028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I will remove the rows in the training data which miss the prediction target (Survived) because such rows will be useless. I will then seperate the target (y) and the predictors (X).","metadata":{}},{"cell_type":"code","source":"X.dropna(axis=0, subset=['Survived'], inplace=True)\ny = X.Survived\nX.drop(['Survived'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T22:17:08.918482Z","iopub.execute_input":"2021-06-12T22:17:08.918783Z","iopub.status.idle":"2021-06-12T22:17:08.926985Z","shell.execute_reply.started":"2021-06-12T22:17:08.918751Z","shell.execute_reply":"2021-06-12T22:17:08.926332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Initially, I will split the training data. I will use 80% of the training data to train the initial model, and I will use this initial model to predict the other 20% of the training data. This way, I can validate the initial model. After validating the model, I will make a final model, which I will train with the full training data. In the cell below, I split the data 80% for initial training, and 20% for validation.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y,\n                                                      train_size=0.8, test_size=0.2,\n                                                      random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T22:17:08.928112Z","iopub.execute_input":"2021-06-12T22:17:08.928531Z","iopub.status.idle":"2021-06-12T22:17:08.940424Z","shell.execute_reply.started":"2021-06-12T22:17:08.928494Z","shell.execute_reply":"2021-06-12T22:17:08.939589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the following cells, I will handle missing values.","metadata":{}},{"cell_type":"markdown","source":"I will start by exploring the data. The following cell outputs the number of the rows and the number of the columns in the training data. And for each column that contains missing value(s), it outputs the number of rows missing.","metadata":{}},{"cell_type":"code","source":"# Shape of training data (num_rows, num_columns)\nprint(X_train.shape)\n\n# Number of missing values in each column of training data\nmissing_val_count_by_column = (X_train.isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])\n","metadata":{"execution":{"iopub.status.busy":"2021-06-12T22:17:08.941884Z","iopub.execute_input":"2021-06-12T22:17:08.942469Z","iopub.status.idle":"2021-06-12T22:17:08.960751Z","shell.execute_reply.started":"2021-06-12T22:17:08.942424Z","shell.execute_reply":"2021-06-12T22:17:08.959591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So there are three columns containing missing values: Age, Cabin, Embarked\n\nLet's begin with the Cabin column. As more than half of the rows are missing, I will choose to drop that column.","metadata":{}},{"cell_type":"code","source":"X_train = X_train.drop(['Cabin'], axis = 1)\nX_valid = X_valid.drop(['Cabin'], axis = 1)\nX_test = X_test.drop(['Cabin'], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T22:17:08.962253Z","iopub.execute_input":"2021-06-12T22:17:08.96257Z","iopub.status.idle":"2021-06-12T22:17:08.976791Z","shell.execute_reply.started":"2021-06-12T22:17:08.962539Z","shell.execute_reply":"2021-06-12T22:17:08.975999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As for the Embarked column, there are only 2 missing values. So I will choose to set them to \"unknown\".","metadata":{}},{"cell_type":"code","source":"X_train.Embarked.fillna(\"unknown\")\nX_valid.Embarked.fillna(\"unknown\")\nX_test.Embarked.fillna(\"unknown\")","metadata":{"execution":{"iopub.status.busy":"2021-06-12T22:17:08.978223Z","iopub.execute_input":"2021-06-12T22:17:08.978786Z","iopub.status.idle":"2021-06-12T22:17:08.991303Z","shell.execute_reply.started":"2021-06-12T22:17:08.978745Z","shell.execute_reply":"2021-06-12T22:17:08.990372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order to decide what to do with the Age column, I will further explore the data by using the describe() function in the pandas module.","metadata":{}},{"cell_type":"code","source":"X_train.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T22:17:08.992631Z","iopub.execute_input":"2021-06-12T22:17:08.992901Z","iopub.status.idle":"2021-06-12T22:17:09.026075Z","shell.execute_reply.started":"2021-06-12T22:17:08.992874Z","shell.execute_reply":"2021-06-12T22:17:09.025047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I can see from the output that there is no big difference between the mean and the median values of the Age column. This means I can replace the missing values by either the mean or the median. The standard deviation of the Age column is also relatively low. So I will choose to fill the missing values in that column with the mean value.\nBut I will leave this now. I will handle the categorical variables first, and I will come back to it.","metadata":{}},{"cell_type":"markdown","source":"In the following cells, I will handle categorical data","metadata":{}},{"cell_type":"markdown","source":"The cell below outputs the names of the categorical columns. And for each categorical variable, it outputs the number of unique values.","metadata":{}},{"cell_type":"code","source":"s = (X_train.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)\n\nobject_nunique = list(map(lambda col: X_train[col].nunique(), object_cols))\nd = dict(zip(object_cols, object_nunique))\n\nsorted(d.items(), key=lambda x: x[1])","metadata":{"execution":{"iopub.status.busy":"2021-06-12T22:17:09.027203Z","iopub.execute_input":"2021-06-12T22:17:09.027489Z","iopub.status.idle":"2021-06-12T22:17:09.038777Z","shell.execute_reply.started":"2021-06-12T22:17:09.027459Z","shell.execute_reply":"2021-06-12T22:17:09.037867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So the categorical columns are: Name, Sex, Ticket, Emberked\n\nI will begin with the Name column. I think it will not affect the result significantly, so I will choose to drop it.","metadata":{}},{"cell_type":"code","source":"X_train = X_train.drop(['Name'], axis = 1)\nX_valid = X_valid.drop(['Name'], axis = 1)\nX_test = X_test.drop(['Name'], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T22:17:09.040567Z","iopub.execute_input":"2021-06-12T22:17:09.040875Z","iopub.status.idle":"2021-06-12T22:17:09.048238Z","shell.execute_reply.started":"2021-06-12T22:17:09.040848Z","shell.execute_reply":"2021-06-12T22:17:09.047214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As for the Sex and the Embarked columns, these two variables are nominal. So one-hot encoding will be a suitable approach. It is also worth noting that, Sex has only two unique values, and Embarked has only three unique values. This means that one-hot encoding these two columns will not add too much entries to the data.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\ncolss = ['Sex', 'Embarked']\n\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[colss]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[colss]))\nOH_cols_test = pd.DataFrame(OH_encoder.transform(X_test[colss]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = X_train.index\nOH_cols_valid.index = X_valid.index\nOH_cols_test.index = X_test.index\n\nother_X_train = X_train.drop(['Sex', 'Embarked'], axis=1)\nother_X_valid = X_valid.drop(['Sex', 'Embarked'], axis=1)\nother_X_test = X_test.drop(['Sex', 'Embarked'], axis=1)\n\nOH_X_train = pd.concat([other_X_train, OH_cols_train], axis=1)\nOH_X_valid = pd.concat([other_X_valid, OH_cols_valid], axis=1)\nOH_X_test = pd.concat([other_X_test, OH_cols_test], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T22:17:09.127561Z","iopub.execute_input":"2021-06-12T22:17:09.127887Z","iopub.status.idle":"2021-06-12T22:17:09.150589Z","shell.execute_reply.started":"2021-06-12T22:17:09.127861Z","shell.execute_reply":"2021-06-12T22:17:09.149563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The remaining categorical variable is: Ticket. It is nominal variable, so label encoding it will be suitable. I will check first whether there are values of Ticket in the validation data that does not appear in the train data.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nif not set(OH_X_valid['Ticket']).issubset(set(OH_X_train['Ticket'])):\n    print(\"Ticket cannot be label encoded\")","metadata":{"execution":{"iopub.status.busy":"2021-06-12T22:17:09.151891Z","iopub.execute_input":"2021-06-12T22:17:09.152129Z","iopub.status.idle":"2021-06-12T22:17:09.157289Z","shell.execute_reply.started":"2021-06-12T22:17:09.152105Z","shell.execute_reply":"2021-06-12T22:17:09.156388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As I cannot label encode the Ticket column, I will drop it.","metadata":{}},{"cell_type":"code","source":"OH_X_train = OH_X_train.drop(['Ticket'], axis = 1)\nOH_X_valid = OH_X_valid.drop(['Ticket'], axis = 1)\nOH_X_test = OH_X_test.drop(['Ticket'], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T22:17:09.15941Z","iopub.execute_input":"2021-06-12T22:17:09.159963Z","iopub.status.idle":"2021-06-12T22:17:09.173388Z","shell.execute_reply.started":"2021-06-12T22:17:09.159924Z","shell.execute_reply":"2021-06-12T22:17:09.172404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now it is time to come back to the Age column. As I said before, I will fill it with the mean value. Note that, now the only column containing missing value is the age column. So imputaion will be done on that column.","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer()\nimputed_X_train = pd.DataFrame(imputer.fit_transform(OH_X_train))\nimputed_X_valid = pd.DataFrame(imputer.transform(OH_X_valid))\nimputed_X_test = pd.DataFrame(imputer.transform(OH_X_test))\n\nimputed_X_train.columns = OH_X_train.columns\nimputed_X_valid.columns = OH_X_valid.columns","metadata":{"execution":{"iopub.status.busy":"2021-06-12T22:17:09.175061Z","iopub.execute_input":"2021-06-12T22:17:09.175486Z","iopub.status.idle":"2021-06-12T22:17:09.192929Z","shell.execute_reply.started":"2021-06-12T22:17:09.175442Z","shell.execute_reply":"2021-06-12T22:17:09.191994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After preprocessing the data; after handling missing values and categorical variables, I will build the model. I choose the random forest classifier model. I will train the model with 80% of the preprocessed training data (imputed_X_train). Then, I will use the model to predict 20% of the preprocessed training data (imputed_X_valid). I will use the predictions to validate the model by calculating the mean absolute error in the predictions.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import mean_absolute_error\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(imputed_X_train, y_train)\npredictions = model.predict(imputed_X_valid)\nmae = mean_absolute_error(predictions, y_valid)\nmae","metadata":{"execution":{"iopub.status.busy":"2021-06-12T22:17:09.194127Z","iopub.execute_input":"2021-06-12T22:17:09.194418Z","iopub.status.idle":"2021-06-12T22:17:09.36798Z","shell.execute_reply.started":"2021-06-12T22:17:09.19439Z","shell.execute_reply":"2021-06-12T22:17:09.366973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The output ranges from 0 to 1. So if the mean absolute error is approximately 0.1731, then the performance of the model can be considered to be good :)","metadata":{}},{"cell_type":"markdown","source":"I also measered the performance of the model using the accuracy_socre() function found in the sklearn.metrics module","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy_score(y_valid, predictions)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T22:17:09.369305Z","iopub.execute_input":"2021-06-12T22:17:09.36969Z","iopub.status.idle":"2021-06-12T22:17:09.37692Z","shell.execute_reply.started":"2021-06-12T22:17:09.369649Z","shell.execute_reply":"2021-06-12T22:17:09.376044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The output of the above cell is approximately 0.8268. So the accuracy of the model is around 82.68% ","metadata":{}},{"cell_type":"markdown","source":"I will make a new model. I will train this new model with the whole training data (rather than training it will only 80% of it, to increase accuracy). Then, I will use this new model to predict the preprocessed test data.","metadata":{}},{"cell_type":"code","source":"final_model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nfinal_model.fit(imputed_X_train, y_train)\nfinal_preds = final_model.predict(imputed_X_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T22:17:09.377957Z","iopub.execute_input":"2021-06-12T22:17:09.378203Z","iopub.status.idle":"2021-06-12T22:17:09.554982Z","shell.execute_reply.started":"2021-06-12T22:17:09.378173Z","shell.execute_reply":"2021-06-12T22:17:09.554116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, I will save the predictions in a csv file in the format required in the competition.\nI added a print statement to make sure the cell runs successfully.","metadata":{}},{"cell_type":"code","source":"output = pd.DataFrame({'PassengerId': X_test.index, 'Survived': final_preds})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{"execution":{"iopub.status.busy":"2021-06-12T22:17:09.556138Z","iopub.execute_input":"2021-06-12T22:17:09.556419Z","iopub.status.idle":"2021-06-12T22:17:09.56411Z","shell.execute_reply.started":"2021-06-12T22:17:09.55639Z","shell.execute_reply":"2021-06-12T22:17:09.563255Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
